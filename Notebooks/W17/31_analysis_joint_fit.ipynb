{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # First raw analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run 20_testing_different_bandwidths.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "subj_name_list =  ['HL','JF25','AB','MP'] #changer et mettre le nom des nouveaux participants\n",
    "subj_name_list =  ['LP',] #changer et mettre le nom des nouveaux participants\n",
    "\n",
    "                \n",
    "subj_data_list = []\n",
    "import json\n",
    "for subject in subj_name_list :\n",
    "    x = json.load(open(os.path.join(data_folder, 'Psychophys_discrim_%s.json' % subject), 'rb'))\n",
    "     #changer et mettre le path du fichier où se trouve les psychophysic data\n",
    "    subj_data_list.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "B_theta_list = []\n",
    "for x in subj_data_list[0] :\n",
    "    B_theta_list.append(x[3]*180/np.pi)\n",
    "B_theta_list = sorted(set(B_theta_list))\n",
    "\n",
    "print('B_theta_list=', B_theta_list)\n",
    "\n",
    "B_sf_list = []\n",
    "for x in subj_data_list[0] :\n",
    "    B_sf_list.append(x[4])\n",
    "B_sf_list = sorted(set(B_sf_list))\n",
    "print('B_sf_list=', B_sf_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theta/Btheta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib.pyplot import cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B_sf/ B_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {}\n",
    "\n",
    "for num, data in enumerate(subj_data_list) :\n",
    "    data_dict[num] = {}\n",
    "    N_data = len(data)\n",
    "    data_dict[num]['theta'] = np.zeros((1, N_data))\n",
    "    data_dict[num]['conditions'] = np.zeros((1, N_data)).astype(np.int)\n",
    "    data_dict[num]['response'] = np.zeros((1, N_data))\n",
    "    for i, item in enumerate(data):\n",
    "        #print(i, item)\n",
    "        data_dict[num]['theta'][0, i] = item[1]*180/np.pi\n",
    "        data_dict[num]['conditions'][0, i] = item[5]+item[6]*len(B_theta_list)\n",
    "        data_dict[num]['response'][0, i] = 1 if item[2] == 'right' else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "torch.set_default_tensor_type(\"torch.DoubleTensor\")\n",
    "criterion = torch.nn.BCELoss(reduction=\"sum\")\n",
    "\n",
    "N_conditions = len(B_sf_list)*len(B_theta_list)\n",
    "bias = True\n",
    "logit0_init, theta0_init, log_wt_init = -np.log(1/.10 - 1), 0.0, np.log(4.)\n",
    "\n",
    "class LogisticRegressionModel(torch.nn.Module):\n",
    "    def __init__(self, logit0_init, theta0_init, log_wt_init, bias=True):\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        if bias:\n",
    "            self.theta0 = torch.nn.Parameter(theta0_init)\n",
    "        else:\n",
    "            self.theta0 = theta0_init\n",
    "        self.logit0 = torch.nn.Parameter(logit0_init)\n",
    "        self.log_wt = torch.nn.Parameter(log_wt_init)\n",
    "        self.do_indep = self.logit0.shape.numel() > 1\n",
    "        \n",
    "    def forward(self, theta, i_condition):\n",
    "        if self.do_indep:\n",
    "            p0 = torch.sigmoid(self.logit0[i_condition.to(int)])\n",
    "        else:\n",
    "            p0 = torch.sigmoid(self.logit0)\n",
    "            \n",
    "        theta0 = self.theta0[i_condition.to(int)]\n",
    "        wt = torch.exp(self.log_wt[i_condition.to(int)])\n",
    "        out = p0 / 2 + (1 - p0) * torch.sigmoid((theta-theta0)/wt)\n",
    "        return out\n",
    "\n",
    "learning_rate = 0.005\n",
    "beta1, beta2 = 0.9, 0.999\n",
    "betas = (beta1, beta2)\n",
    "num_epochs = 2 ** 12 + 1\n",
    "\n",
    "def fit_data(\n",
    "    theta, ind_condition, y,\n",
    "    logit0_init=logit0_init * torch.ones(1), theta0_init=theta0_init*torch.ones(N_conditions), \n",
    "    log_wt_init=torch.log(log_wt_init*torch.ones(N_conditions)), \n",
    "    bias=bias,\n",
    "    learning_rate=learning_rate,\n",
    "    # batch_size=batch_size,  # previous notebook showed that learning_rate had no influence on performance\n",
    "    num_epochs=num_epochs,\n",
    "    betas=betas,\n",
    "    verbose=False, **kwargs\n",
    "):\n",
    "\n",
    "    Theta, Ind_condition, labels = torch.Tensor(theta[:, None]), torch.ByteTensor(ind_condition[:, None]), torch.Tensor(y[:, None])\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    Theta, Ind_condition, labels = Theta.to(device), Ind_condition.to(device), labels.to(device)        \n",
    "    \n",
    "    logistic_model = LogisticRegressionModel(logit0_init, theta0_init, log_wt_init, bias=bias)\n",
    "    logistic_model = logistic_model.to(device)\n",
    "    logistic_model.train()\n",
    "    optimizer = torch.optim.Adam(logistic_model.parameters(), lr=learning_rate, betas=betas)\n",
    "            \n",
    "    for epoch in range(int(num_epochs)):\n",
    "        outputs = logistic_model(Theta, Ind_condition)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if verbose and (epoch % (num_epochs // 32) == 0):\n",
    "            print(f\"Iteration: {epoch} - Loss: {loss.item() /len(theta):.5f}\")\n",
    "            losses = []\n",
    "\n",
    "    logistic_model.eval()\n",
    "    Theta, Ind_condition, labels = torch.Tensor(theta[:, None]), torch.Tensor(ind_condition[:, None]), torch.Tensor(y[:, None])\n",
    "    outputs = logistic_model(Theta, Ind_condition)\n",
    "    loss = criterion(outputs, labels).item() / len(theta)\n",
    "    return logistic_model, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias = False\n",
    "#bias = True\n",
    "results = {}\n",
    "for num, data in enumerate(subj_data_list) :\n",
    "    logistic_model, loss = fit_data(data_dict[num]['theta'], \n",
    "                                    data_dict[num]['conditions'], \n",
    "                                    data_dict[num]['response'], bias=bias, verbose=True)\n",
    "    results[num] = logistic_model\n",
    "    print(\"Final loss =\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def npa2str(npa):\n",
    "    \"\"\"\n",
    "    format a numpy array into a string\n",
    "    \"\"\"\n",
    "    return ', '.join(list(map('{:.3f}'.format, npa)))\n",
    "\n",
    "def get_params(logistic_model, verbose=False):\n",
    "    theta0_ = logistic_model.theta0.detach().numpy()\n",
    "    wt_ = torch.exp(logistic_model.log_wt).detach().numpy()\n",
    "    p0_ = torch.sigmoid(logistic_model.logit0).detach().numpy()\n",
    "    N_conditions = wt_.shape[0]\n",
    "    if verbose:\n",
    "        #for i_condition in range(N_conditions):\n",
    "        #    print(f'-> i_condition={i_condition}')\n",
    "        #    if bias:\n",
    "        #        print(f\"theta0 = {theta0_[i_condition]:.3f}\")\n",
    "        #    print(f\"slope = {wt_[i_condition]:.3f}\")\n",
    "        if bias:\n",
    "            print(f\"theta0 = {npa2str(theta0_)}\")\n",
    "        print(f\"slope = {npa2str(wt_)}\")\n",
    "        print(f\"p0 = {npa2str(p0_)}\")\n",
    "    return theta0_, wt_, p0_\n",
    "\n",
    "for num, data in enumerate(subj_data_list) :\n",
    "    print('--> observer num', num)\n",
    "    theta0_, wt_, p0_ = get_params(results[num], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num, data in enumerate(subj_data_list) :\n",
    "    print('--> observer num', num)\n",
    "    fig, axs = plt.subplots(len(B_sf_list), 1, figsize=(15, 21))\n",
    "    x_values = np.linspace(-25, 25, 400)\n",
    "\n",
    "    for i_B_sf, B_sf in enumerate(B_sf_list):\n",
    "        ax = axs[i_B_sf]\n",
    "\n",
    "        #colors = plt.cm.inferno(np.linspace(1, .2, len(B_thetas))) #tc colormap\n",
    "\n",
    "        for i_B_theta, B_theta in enumerate(B_theta_list):\n",
    "            i_condition = i_B_theta + i_B_sf*len(B_theta_list)\n",
    "            conditions = i_condition*np.ones_like(x_values)\n",
    "            y_values = results[num](torch.Tensor(x_values), \n",
    "                                      torch.ByteTensor(conditions)).detach().numpy()\n",
    "            ax.plot(x_values, y_values, label=r'$B_\\theta=%s$' % f'{B_theta:.1f}')\n",
    "\n",
    "        ax.set_xlabel(r\"orientation $\\theta$\", fontsize=20)\n",
    "        ax.set_yticks([0.0, 1.0])\n",
    "        ax.set_yticklabels([\"Left\", \"Right\"], fontsize=20)\n",
    "        ax.legend(fontsize=20, frameon=False, scatterpoints=6)\n",
    "        #if i_B_sf==0: ax.legend(fontsize=20, frameon=False, scatterpoints=6)\n",
    "        ax.text(14, 0.1, f'--> B_sf={B_sf:.3f}', fontsize=24)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num, data in enumerate(subj_data_list) :\n",
    "    print('--> observer num', num)\n",
    "    theta0_, wt_, p0_ = get_params(results[num], verbose=True)\n",
    "    coef_list = []\n",
    "    plt.figure(figsize = (8,6)) \n",
    "    for i_B_sf, B_sf in enumerate(B_sf_list):\n",
    "        wt__ = wt_[(i_B_sf*len(B_theta_list)):((i_B_sf+1)*len(B_theta_list))]\n",
    "        plt.plot(B_theta_list, np.log(wt__), label='B_sf = %s' % B_sf)\n",
    "    plt.xlabel(r' $B_\\theta$ (°)')\n",
    "    plt.ylabel(r' log slopes ')\n",
    "    plt.legend(loc ='best') \n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "nteract": {
   "version": "0.21.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
