{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # First raw analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_folder = ../Psychopy_data/pilot/B_theta_B_sf\n",
      "Overwriting tmp/30_testing_different_bandwidths.py\n",
      "\u001b[22;0t\u001b]0;IPython: Notebooks/W17\u0007\n",
      " Initializing\n",
      "B_thetas =  [0.09817477 0.18005332 0.33021925 0.60562477 1.11072073]\n",
      "B_sfs =  [0.0625 0.125  0.25   0.5    1.    ]\n"
     ]
    }
   ],
   "source": [
    "%run 30_testing_different_bandwidths.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_name_list =  ['LP', 'JF']\n",
    "subj_name_list =  ['LP', ]\n",
    "\n",
    "subj_data_list = []\n",
    "import json\n",
    "for subject in subj_name_list :\n",
    "    x = json.load(open(os.path.join(data_folder, 'Psychophys_discrim_%s.json' % subject), 'rb'))\n",
    "     #changer et mettre le path du fichier où se trouve les psychophysic data\n",
    "    subj_data_list.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B_theta_list= [5.625, 10.316295486052553, 18.920169343208578, 34.69974357167867, 63.63961030678928]\n",
      "B_sf_list= [0.0625, 0.125, 0.25, 0.5, 1.0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "B_theta_list = []\n",
    "for x in subj_data_list[0] :\n",
    "    B_theta_list.append(x[3]*180/np.pi)\n",
    "B_theta_list = sorted(set(B_theta_list))\n",
    "\n",
    "print('B_theta_list=', B_theta_list)\n",
    "B_sf_list = []\n",
    "for x in subj_data_list[0] :\n",
    "    B_sf_list.append(x[4])\n",
    "B_sf_list = sorted(set(B_sf_list))\n",
    "print('B_sf_list=', B_sf_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B_sf/ B_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {}\n",
    "\n",
    "for num, data in enumerate(subj_data_list) :\n",
    "    data_dict[num] = {}\n",
    "    N_data = len(data)\n",
    "    data_dict[num]['theta'] = np.zeros((1, N_data))\n",
    "    data_dict[num]['conditions'] = np.zeros((1, N_data)).astype(np.int)\n",
    "    data_dict[num]['response'] = np.zeros((1, N_data))\n",
    "    for i, item in enumerate(data):\n",
    "        #print(i, item)\n",
    "        data_dict[num]['theta'][0, i] = item[1]*180/np.pi\n",
    "        data_dict[num]['conditions'][0, i] = item[5]+item[6]*len(B_theta_list)\n",
    "        data_dict[num]['response'][0, i] = 1 if item[2] == 'right' else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "torch.set_default_tensor_type(\"torch.DoubleTensor\")\n",
    "criterion = torch.nn.BCELoss(reduction=\"sum\")\n",
    "\n",
    "N_conditions = len(B_sf_list)*len(B_theta_list)\n",
    "bias = True\n",
    "logit0_init, theta0_init, log_wt_init = -np.log(1/.10 - 1), 0.0, np.log(4.)\n",
    "\n",
    "class LogisticRegressionModel(torch.nn.Module):\n",
    "    def __init__(self, logit0_init, theta0_init, log_wt_init, bias=True):\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        if bias:\n",
    "            self.theta0 = torch.nn.Parameter(theta0_init)\n",
    "        else:\n",
    "            self.theta0 = theta0_init\n",
    "        self.logit0 = torch.nn.Parameter(logit0_init)\n",
    "        self.log_wt = torch.nn.Parameter(log_wt_init)\n",
    "        self.do_indep = self.logit0.shape.numel() > 1\n",
    "        \n",
    "    def forward(self, theta, i_condition):\n",
    "        if self.do_indep:\n",
    "            p0 = torch.sigmoid(self.logit0[i_condition.to(int)])\n",
    "        else:\n",
    "            p0 = torch.sigmoid(self.logit0)\n",
    "            \n",
    "        theta0 = self.theta0[i_condition.to(int)]\n",
    "        wt = torch.exp(self.log_wt[i_condition.to(int)])\n",
    "        out = p0 / 2 + (1 - p0) * torch.sigmoid((theta-theta0)/wt)\n",
    "        return out\n",
    "\n",
    "learning_rate = 0.005\n",
    "beta1, beta2 = 0.9, 0.999\n",
    "betas = (beta1, beta2)\n",
    "num_epochs = 2 ** 12 + 1\n",
    "\n",
    "def fit_data(logistic_model,\n",
    "    theta, ind_condition, y,\n",
    "    learning_rate=learning_rate,\n",
    "    # batch_size=batch_size,  # previous notebook showed that learning_rate had no influence on performance\n",
    "    num_epochs=num_epochs,\n",
    "    betas=betas,\n",
    "    verbose=False, **kwargs\n",
    "):\n",
    "\n",
    "    Theta, Ind_condition, labels = torch.Tensor(theta[:, None]), torch.ByteTensor(ind_condition[:, None]), torch.Tensor(y[:, None])\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    Theta, Ind_condition, labels = Theta.to(device), Ind_condition.to(device), labels.to(device)        \n",
    "    \n",
    "    logistic_model = logistic_model.to(device)\n",
    "    logistic_model.train()\n",
    "    optimizer = torch.optim.Adam(logistic_model.parameters(), lr=learning_rate, betas=betas)\n",
    "            \n",
    "    for epoch in range(int(num_epochs)):\n",
    "        outputs = logistic_model(Theta, Ind_condition)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if verbose and (epoch % (num_epochs // 32) == 0):\n",
    "            print(f\"Iteration: {epoch} - Loss: {loss.item() /len(theta):.5f}\")\n",
    "            losses = []\n",
    "\n",
    "    logistic_model.eval()\n",
    "    Theta, Ind_condition, labels = torch.Tensor(theta[:, None]), torch.Tensor(ind_condition[:, None]), torch.Tensor(y[:, None])\n",
    "    outputs = logistic_model(Theta, Ind_condition)\n",
    "    loss = criterion(outputs, labels).item() / len(theta)\n",
    "    logistic_model.cpu()\n",
    "    return logistic_model, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 - Loss: 316.55038\n",
      "Iteration: 128 - Loss: 269.11273\n",
      "Iteration: 256 - Loss: 242.51238\n",
      "Iteration: 384 - Loss: 231.57964\n",
      "Iteration: 512 - Loss: 227.50920\n",
      "Iteration: 640 - Loss: 225.73848\n",
      "Iteration: 768 - Loss: 224.87052\n",
      "Iteration: 896 - Loss: 224.42057\n",
      "Iteration: 1024 - Loss: 224.17352\n",
      "Iteration: 1152 - Loss: 224.03040\n",
      "Iteration: 1280 - Loss: 223.94324\n",
      "Iteration: 1408 - Loss: 223.88658\n",
      "Iteration: 1536 - Loss: 223.84263\n",
      "Iteration: 1664 - Loss: 223.59224\n",
      "Iteration: 1792 - Loss: 223.52987\n",
      "Iteration: 1920 - Loss: 223.50709\n",
      "Iteration: 2048 - Loss: 223.49432\n",
      "Iteration: 2176 - Loss: 223.48545\n",
      "Iteration: 2304 - Loss: 223.47879\n",
      "Iteration: 2432 - Loss: 223.47363\n",
      "Iteration: 2560 - Loss: 223.46955\n",
      "Iteration: 2688 - Loss: 223.46629\n",
      "Iteration: 2816 - Loss: 223.46365\n",
      "Iteration: 2944 - Loss: 223.46147\n",
      "Iteration: 3072 - Loss: 223.45967\n",
      "Iteration: 3200 - Loss: 223.45815\n",
      "Iteration: 3328 - Loss: 223.45687\n",
      "Iteration: 3456 - Loss: 223.45578\n",
      "Iteration: 3584 - Loss: 223.45484\n",
      "Iteration: 3712 - Loss: 223.45402\n",
      "Iteration: 3840 - Loss: 223.45331\n",
      "Iteration: 3968 - Loss: 223.45270\n",
      "Iteration: 4096 - Loss: 223.45216\n",
      "Final loss = 223.45215426790125\n"
     ]
    }
   ],
   "source": [
    "bias = False\n",
    "#bias = True\n",
    "logit0_init = logit0_init * torch.ones(1)\n",
    "# indep = logit0_init=logit0_init * torch.ones(int(len(data)/len(B_theta_list)/len(B_sf_list)))\n",
    "results = {}\n",
    "for num, data in enumerate(subj_data_list) :\n",
    "    logistic_model = LogisticRegressionModel(logit0_init * torch.ones(1), theta0_init*torch.ones(N_conditions), torch.log(log_wt_init*torch.ones(N_conditions)), bias=bias)\n",
    "    logistic_model, loss = fit_data(logistic_model, data_dict[num]['theta'], \n",
    "                                    data_dict[num]['conditions'], \n",
    "                                    data_dict[num]['response'], verbose=True)\n",
    "    results[num] = logistic_model\n",
    "    print(\"Final loss =\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([-1.8153], requires_grad=True)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-41d5fd95e84e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogit0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 1"
     ]
    }
   ],
   "source": [
    "for num in range(2): print(results[num].logit0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def npa2str(npa):\n",
    "    \"\"\"\n",
    "    format a numpy array into a string\n",
    "    \"\"\"\n",
    "    return ', '.join(list(map('{:.3f}'.format, npa)))\n",
    "\n",
    "def get_params(logistic_model, verbose=False):\n",
    "    theta0_ = logistic_model.theta0.detach().numpy()\n",
    "    wt_ = torch.exp(logistic_model.log_wt).detach().numpy()\n",
    "    p0_ = torch.sigmoid(logistic_model.logit0).detach().numpy()\n",
    "    N_conditions = wt_.shape[0]\n",
    "    if verbose:\n",
    "        #for i_condition in range(N_conditions):\n",
    "        #    print(f'-> i_condition={i_condition}')\n",
    "        #    if bias:\n",
    "        #        print(f\"theta0 = {theta0_[i_condition]:.3f}\")\n",
    "        #    print(f\"slope = {wt_[i_condition]:.3f}\")\n",
    "        if bias:\n",
    "            print(f\"theta0 = {npa2str(theta0_)}\")\n",
    "        print(f\"slope = {npa2str(wt_)}\")\n",
    "        print(f\"p0 = {npa2str(p0_)}\")\n",
    "    return theta0_, wt_, p0_\n",
    "\n",
    "for num, data in enumerate(subj_data_list) :\n",
    "    print('--> observer num', num)\n",
    "    theta0_, wt_, p0_ = get_params(results[num], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(len(B_sf_list), len(subj_data_list), figsize=(15, 21))\n",
    "for num, data in enumerate(subj_data_list) :\n",
    "    print('--> observer num', num)\n",
    "    x_values = np.linspace(-25, 25, 400)\n",
    "\n",
    "    for i_B_sf, B_sf in enumerate(B_sf_list):\n",
    "        if len(subj_data_list)>1:\n",
    "            ax = axs[i_B_sf][num]\n",
    "        else:\n",
    "            ax = axs[i_B_sf]\n",
    "\n",
    "        #colors = plt.cm.inferno(np.linspace(1, .2, len(B_thetas))) #tc colormap\n",
    "\n",
    "        for i_B_theta, B_theta in enumerate(B_theta_list):\n",
    "            i_condition = i_B_theta + i_B_sf*len(B_theta_list)\n",
    "            conditions = i_condition*np.ones_like(x_values)\n",
    "            y_values = results[num](torch.Tensor(x_values), \n",
    "                                      torch.ByteTensor(conditions)).detach().numpy()\n",
    "            ax.plot(x_values, y_values, label=r'$B_\\theta=%s$' % f'{B_theta:.1f}')\n",
    "\n",
    "        ax.set_xlabel(r\"orientation $\\theta$ (°)\", fontsize=12)\n",
    "        ax.set_yticks([0.0, 1.0])\n",
    "        ax.set_yticklabels([\"Left\", \"Right\"], fontsize=12)\n",
    "        ax.legend(fontsize=11, frameon=False, scatterpoints=6)\n",
    "        if num==0:  ax.legend(fontsize=11, frameon=False, scatterpoints=6)\n",
    "        ax.text(14, 0.1, f'B_sf = {B_sf:.3f}', fontsize=12)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, len(subj_data_list), figsize=(8,6)) \n",
    "for num, data in enumerate(subj_data_list) :\n",
    "    print('--> observer num', num)\n",
    "    theta0_, wt_, p0_ = get_params(results[num], verbose=True)\n",
    "    coef_list = []\n",
    "    for i_B_sf, B_sf in enumerate(B_sf_list):\n",
    "        wt__ = wt_[(i_B_sf*len(B_theta_list)):((i_B_sf+1)*len(B_theta_list))]\n",
    "        axs[num].plot(B_theta_list, np.log(wt__), label=f'B_sf = {B_sf:.3f}')\n",
    "    axs[num].set_xlabel(r' $B_\\theta$ (°)')\n",
    "    axs[num].set_ylabel(r' log slopes ')\n",
    "    axs[num].legend(loc ='best') \n",
    "    axs[num].set_xscale('log')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "nteract": {
   "version": "0.21.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
